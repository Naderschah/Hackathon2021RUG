{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The SAFE Specification\n",
    "\n",
    "Sentinel data products use a variation of the Standard Archive Format for Europe (SAFE)\n",
    "\n",
    "The SENITNEL-SAFE format wraps a folder containing image data and metadata in XML.\n",
    "\n",
    "A SENTINEL Product then refers to a folder containing:\n",
    "- a 'manifest.safe' file which holds the general product information in XML\n",
    "    - This includes file information and observation metadata (i.e. observation mode)\n",
    "- subfolders for measurement datasets containing image data in various binary formats\n",
    "    - .dat for raw data, .tiff and .jp2 for higher level data\n",
    "- a preview folder containing 'quicklooks' in PNG format, Google Earth overlays in KML format and HTML preview files\n",
    "- an annotation folder containing the product metadata in XML as well as calibration data\n",
    "    - The top level xml files contain information such as average noise, orbital data, beam data, etc.\n",
    "    - The xml files within calibration contain calibration vectors and information about the errors\n",
    "        - How exactly these are applied you will have to check in the technical guide\n",
    "- a support folder containing the XML schemes describing the product XML.\n",
    "    - The text within it can be used to understand the calibration data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentinel-1\n",
    "\n",
    "### Level-0\n",
    "\n",
    "The data files contain the raw measurement data in the form of a stream of downlinked ISPs.\n",
    "\n",
    "The binary data is stored in big-endian format\n",
    "\n",
    "### Level-1\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Opening the manifest, support and anotation files\n",
    "\n",
    "## XML\n",
    "\n",
    "XML is the extensible Markup Language\n",
    "\n",
    "## XSD\n",
    "\n",
    "XSD  is the XML Schema definition, this document expresses the constraints for the XML documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some example files\n",
    "\n",
    "## manifest.SAFE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# To open XML files we will be using BeautifulSoup (this is not a requirement but does make filehandling easier)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#A helper function to print reduced output\n",
    "def reduce_out(i,len=500):\n",
    "    print(str(i)[:len:]+'...')\n",
    "\n",
    "\n",
    "with open('S1B_IW_SLC__1SSH_20210717T014251_20210717T014306_027827_03520D_4B30.SAFE/manifest.safe') as f:\n",
    "    manifest = f.read()\n",
    "\n",
    "manifest = BeautifulSoup(manifest,'xml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "# Within the manifest there are several delimiters\n",
    "# the first is the Information Package Map,\n",
    "\n",
    "reduce_out(manifest.informationPackageMap)\n",
    "\n",
    "# This contains general file information"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<informationPackageMap>\n",
      "<xfdu:contentUnit dmdID=\"acquisitionPeriod platform generalProductInformation measurementOrbitReference measurementFrameSet\" pdiID=\"processing\" textInfo=\"Sentinel-1 IW Level-1 SLC Product\" unitType=\"SAFE Archive Information Package\">\n",
      "<xfdu:contentUnit repID=\"s1Level1ProductSchema\" unitType=\"Metadata Unit\">\n",
      "<dataObjectPointer dataObjectID=\"products1biw1slchh20210717t01425320210717t01430402782703520d001\"/>\n",
      "</xfdu:contentUnit>\n",
      "<xfdu:contentUnit repID=\"s1Level1NoiseSchema\" un...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "# Then there is the metadataSection\n",
    "\n",
    "reduce_out(manifest.metadataSection)\n",
    "\n",
    "# This contains metadata Object ID's, Post Processing information (location, time, facilities), and much more"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<metadataSection>\n",
      "<metadataObject ID=\"products1biw1slchh20210717t01425320210717t01430402782703520d001Annotation\" category=\"DMD\" classification=\"DESCRIPTION\">\n",
      "<dataObjectPointer dataObjectID=\"products1biw1slchh20210717t01425320210717t01430402782703520d001\"/>\n",
      "</metadataObject>\n",
      "<metadataObject ID=\"noises1biw1slchh20210717t01425320210717t01430402782703520d001Annotation\" category=\"DMD\" classification=\"DESCRIPTION\">\n",
      "<dataObjectPointer dataObjectID=\"noises1biw1slchh20210717t01425320210717t0143040278270...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "# And lastly we have the dataObjectSection\n",
    "reduce_out(manifest.dataObjectSection)\n",
    "\n",
    "# This contains DataObjectIDs the filenames of the individual xml and xsd files with the repID, an identifier such as s1Level1CalibrationSchema,\n",
    "# the relative file path, and the MD5 checksum (used to verify downloads)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<dataObjectSection>\n",
      "<dataObject ID=\"products1biw1slchh20210717t01425320210717t01430402782703520d001\" repID=\"s1Level1ProductSchema\">\n",
      "<byteStream mimeType=\"text/xml\" size=\"428278\">\n",
      "<fileLocation href=\"./annotation/s1b-iw1-slc-hh-20210717t014253-20210717t014304-027827-03520d-001.xml\" locatorType=\"URL\"/>\n",
      "<checksum checksumName=\"MD5\">4e1e15bc8e760f1424ce80eb13250e8b</checksum>\n",
      "</byteStream>\n",
      "</dataObject>\n",
      "<dataObject ID=\"noises1biw1slchh20210717t01425320210717t01430402782703520d001\" repID=\"s1Level1Noi...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "# Say we want to get the footprint, we will use a dictionary with the ID key and the relevant identifier for the footprint\n",
    "manifest.metadataSection.find('metadataObject',{'ID':'measurementFrameSet'})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<metadataObject ID=\"measurementFrameSet\" category=\"DMD\" classification=\"DESCRIPTION\">\n",
       "<metadataWrap mimeType=\"text/xml\" textInfo=\"Frame Set\" vocabularyName=\"SAFE\">\n",
       "<xmlData>\n",
       "<safe:frameSet>\n",
       "<safe:frame>\n",
       "<safe:footPrint srsName=\"http://www.opengis.net/gml/srs/epsg.xml#4326\">\n",
       "<gml:coordinates>-8.098805,59.837269 -7.596385,57.602821 -6.688732,57.807568 -7.187776,60.036827</gml:coordinates>\n",
       "</safe:footPrint>\n",
       "</safe:frame>\n",
       "</safe:frameSet>\n",
       "</xmlData>\n",
       "</metadataWrap>\n",
       "</metadataObject>"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# We now have the correct metadataObject group, and extract the coordinates\n",
    "\n",
    "coord = manifest.metadataSection.find('metadataObject',{'ID':'measurementFrameSet'}).find('gml:coordinates')\n",
    "\n",
    "print('Coord group:')\n",
    "print(coord)\n",
    "\n",
    "# The content within this xml file is no longer in xml format so we need to do the rest manually\n",
    "# Now to only get the data we can do:\n",
    "\n",
    "coord = coord.getText()\n",
    "print('Coord data:')\n",
    "print(coord)\n",
    "\n",
    "# And lastly we place it in a list of lists, as this is sequential long, lat data\n",
    "\n",
    "coord = [[float(j) for j in i.split(',')] for i in coord.split(' ')]\n",
    "\n",
    "print('Coord formated:')\n",
    "print(coord)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coord group:\n",
      "<gml:coordinates>-8.098805,59.837269 -7.596385,57.602821 -6.688732,57.807568 -7.187776,60.036827</gml:coordinates>\n",
      "Coord data:\n",
      "-8.098805,59.837269 -7.596385,57.602821 -6.688732,57.807568 -7.187776,60.036827\n",
      "Coord formated:\n",
      "[[-8.098805, 59.837269], [-7.596385, 57.602821], [-6.688732, 57.807568], [-7.187776, 60.036827]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calibration Files\n",
    "\n",
    "Here we will extract some calibration vectors\n",
    "\n",
    "While this will always work for the example product type, this may not directly work for others, always make sure to double check in and output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('S1B_IW_SLC__1SSH_20210717T014251_20210717T014306_027827_03520D_4B30.SAFE/annotation/calibration/calibration-s1b-iw1-slc-hh-20210717t014253-20210717t014304-027827-03520d-001.xml') as f:\n",
    "    iw1_calibration = f.read()\n",
    "\n",
    "iw1_calibration = BeautifulSoup(iw1_calibration,'xml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# If we want the first entry of productType we can do:\n",
    "iw1_calibration.productType\n",
    "# However this only returns the very first entry, this can cause unexpected behavior, try using the full path, or be sure this is the entry you wanted\n",
    "# This for example is not a top level entry, the full path to it would be\n",
    "iw1_calibration.calibration.adsHeader.productType"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<productType>SLC</productType>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Lets get all header data, using children to get a list of all subattributes\n",
    "header = {}\n",
    "for i in iw1_calibration.adsHeader.children:\n",
    "    #We filter out all children without content\n",
    "    if len(i.getText()) != 0 and not i.getText().isspace():\n",
    "        header[i.name] = i.getText()\n",
    "header"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'missionId': 'S1B',\n",
       " 'productType': 'SLC',\n",
       " 'polarisation': 'HH',\n",
       " 'mode': 'IW',\n",
       " 'swath': 'IW1',\n",
       " 'startTime': '2021-07-17T01:42:53.624770',\n",
       " 'stopTime': '2021-07-17T01:43:04.954997',\n",
       " 'absoluteOrbitNumber': '27827',\n",
       " 'missionDataTakeId': '217613',\n",
       " 'imageNumber': '001'}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# The next section is calibrationInformation, it only has one entry:\n",
    "iw1_calibration.calibration.calibrationInformation"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<calibrationInformation>\n",
       "<absoluteCalibrationConstant>1.393000e+00</absoluteCalibrationConstant>\n",
       "</calibrationInformation>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# To retrieve the last entry group the calibration vectors we can do\n",
    "reduce_out(iw1_calibration.calibration.calibrationVectorList)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<calibrationVectorList count=\"18\">\n",
      "<calibrationVector>\n",
      "<azimuthTime>2021-07-17T01:42:51.809714</azimuthTime>\n",
      "<line>-1029</line>\n",
      "<pixel count=\"518\">0 40 80 120 160 200 240 280 320 360 400 440 480 520 560 600 640 680 720 760 800 840 880 920 960 1000 1040 1080 1120 1160 1200 1240 1280 1320 1360 1400 1440 1480 1520 1560 1600 1640 1680 1720 1760 1800 1840 1880 1920 1960 2000 2040 2080 2120 2160 2200 2240 2280 2320 2360 2400 2440 2480 2520 2560 2600 2640 2680 2720 2760 2800 2840 2880 2920 2960 3000 30...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# Below I will define a function to extract all elements of a calibration vector and transform dates into datetime objects, floats into floats, and lists into numpy arrays of floats\n",
    "def extract_cal_vector(cal_vector):\n",
    "    # The \"\"\" text \"\"\" is a docstring, always add docstrings\n",
    "    # They should include the functions purpose and a descriprion of in and (out) -put\n",
    "    # How you format this is up to you, it should be pythonic however, so readable and understandable over all else\n",
    "    \"\"\" Extracts data from xml calibration vector entry\n",
    "    ----------------------------------------------------\n",
    "        input:\n",
    "    param: cal_vector (bs4.element.Tag) ---> xml vector endry as returned by bs4.BeautifulSoup\n",
    "        \n",
    "        output:\n",
    "    param: calibration (dict) ---> dictionary containing xml name fields with corresponding formated text\n",
    "    \"\"\"\n",
    "    calibration = {}\n",
    "    # We will use the .children attribute to extract each individual element in the form <identifier>content</identifier>\n",
    "    # Then we will add all calibration vectors to a dictionary object for easier use\n",
    "    for i  in cal_vector.children:\n",
    "        # First we check if the child has content or is just newlines and whitespaces, \n",
    "        # note that i is not a string object but a bs4 Tag object\n",
    "        if len(i.getText()) != 0 and not i.getText().isspace(): \n",
    "            # We get the name of the identifier\n",
    "            identifier = i.name\n",
    "            # Next we get the associated text\n",
    "            cont = i.getText()\n",
    "            # Now we will format the loaded data and assign it to our dictionary object\n",
    "            if 'T' in cont:\n",
    "                # If the object is a time object we load it as a datetime object\n",
    "                calibration[identifier] = dt.datetime.strptime(cont, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "            elif len(cont.split(' ')) == 1:\n",
    "                # If it is a singular value\n",
    "                calibration[identifier] = float(cont)\n",
    "            else:\n",
    "                # Otherwise we place the items in a numpy array for further use (all data is seperated using a single whitespace)\n",
    "                calibration[identifier] = np.fromstring(cont, sep=' ',dtype=float)\n",
    "    return calibration"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# We can extract all calibration vectors using:\n",
    "cal_vectors = iw1_calibration.calibration.calibrationVectorList.find_all('calibrationVector')\n",
    "# Now we can iterate over the calibration vectors to extract their data\n",
    "cal_vectors_dict = {}\n",
    "count = 1 \n",
    "# We will index the calibration vectors using number indices, the exact meaning of this must be extracted from the technical and user guide \n",
    "for i in cal_vectors:\n",
    "    # Extract all attributes\n",
    "    cal_vectors_dict[count] = extract_cal_vector(i)\n",
    "    # And add one to count\n",
    "    count += 1\n",
    "\n",
    "del cal_vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# We can then check how many calibration vectors we have\n",
    "cal_vectors_dict.keys()\n",
    "# As we can see the max index 18 matches the count as returned to us above"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Each cal vector then has the attributes\n",
    "cal_vectors_dict[1].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['azimuthTime', 'line', 'pixel', 'sigmaNought', 'betaNought', 'gamma', 'dn'])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#A helper function to print reduced output\n",
    "def reduce_out(i,len=500):\n",
    "    print(str(i)[:len:]+'...')\n",
    "\n",
    "\n",
    "with open('S1B_IW_SLC__1SSH_20210717T014251_20210717T014306_027827_03520D_4B30.SAFE/support/s1-level-1-noise.xsd') as f:\n",
    "    support_noise = f.read()\n",
    "\n",
    "support_noise = BeautifulSoup(support_noise,'xml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# Now we create a nested dictionary object to hold all the data\n",
    "support_noise_dict = {}\n",
    "# We will extract all elements complexType\n",
    "for i in support_noise.schema.find_all('complexType'):\n",
    "    # Then we get the children of each complexType and define a temporary dict to assign each parameter to \n",
    "    tmp_dict = {}\n",
    "    for j in i.children:\n",
    "        #Filter out empty entries\n",
    "        if len(j.getText()) != 0 and not j.getText().isspace(): \n",
    "            # There is a lot of newlines in here so we will do some formating\n",
    "            # Split along newlines\n",
    "            text = j.getText().split('\\n')\n",
    "            # Filter empty lines\n",
    "            text = [k for k in text if len(k)>0]\n",
    "            # recombine non empty strings\n",
    "            text = '\\n'.join(text)\n",
    "            #assign to temporary dict\n",
    "            tmp_dict[j.name] = text\n",
    "    # Now we assign the tmp_dict to our final dictionary, as its name we will be using the groups name\n",
    "    support_noise_dict[i.get('name')]= tmp_dict\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# Now we have our dict\n",
    "support_noise_dict.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['noiseRangeVectorType', 'noiseRangeVectorListType', 'noiseAzimuthVectorType', 'noiseAzimuthVectorListType', 'l1NoiseVectorType'])"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "support_noise_dict['noiseRangeVectorType'].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['annotation', 'sequence'])"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "print(support_noise_dict['noiseRangeVectorType']['annotation'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Annotation record for range noise vectors.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "print(support_noise_dict['noiseRangeVectorType']['sequence'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Zero Doppler azimuth time at which noise vector applies [UTC].\n",
      "Image line at which the noise vector applies.\n",
      "Image pixel at which the noise vector applies. This array contains the count attribute number of integer values (i.e. one value per point in the noise vector), separated by spaces. The maximum length of this array will be one value for every pixel in an image line, however in general the vectors will be subsampled.\n",
      "Range thermal noise correction vector power values. This array contains the count attribute number of floating point values separated by spaces. \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes the introduction to xml, all methods shown above are generally applicable however will require some modification based on the data your using\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "64ad0ea8d629f00d8168fc91b44a5de2d94c414bf842e4bc86698867624ffac1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}